# Auto Vectorization & HPC Roadmap

**Objective:** Transform Vex into a native data-parallel language by leveraging LLVM's vectorizer and Vex's strict ownership model.

## 1. The Vex Advantage: "Safe Aliasing"
Unlike C/C++, Vex's Borrow Checker guarantees that mutable references (`&mut [T]!`) are unique. This means we can emit `noalias` metadata with 100% confidence, unlocking aggressive optimizations that C++ compilers are too afraid to perform (due to potential pointer aliasing).

## Phase 1: LLVM Metadata Injection (Immediate Win)

LLVM's loop vectorizer is powerful effectively if "hinted" correctly. We need to inject metadata into the back-edge branches of our loops.

### Target Files
- `vex-compiler/src/codegen_ast/statements/loops/for_in_loop.rs`
  - `compile_for_in_array_impl`
  - `compile_for_in_vec_impl`

### Implementation Detail
When building the branch instruction back to the loop condition (`br label %cond`), we append metadata:

```llvm
!0 = !{!"llvm.loop.vectorize.enable", i1 1}
!1 = !{!"llvm.loop.unroll.enable", i1 1}
!2 = !{!"llvm.loop.vectorize.width", i32 4} ; Optional hint
```

**Action Plan:**
1.  Define a helper `add_loop_metadata` in `ASTCodeGen`.
2.  Apply this to `Array` and `Vec` iterators.
3.  Verify IR output contains the metadata.

## Phase 2: The "No-Alias" Guarantee

To allow the vectorizer to load/store multiple elements in parallel without fear of overlapping memory:

1.  **Scope Identifiers:** Create a distinct `alias.scope` for the source and destination arrays in the loop.
2.  **Instruction Tagging:** Tag every `load` and `store` instruction in the loop body with:
    - `!noalias`: "This pointer does not overlap with [other scope]"
    - `!alias.scope`: "This pointer belongs to [this scope]"

Since Vex ensures `A` and `B` do not overlap if mutably borrowed, this is safe.

## Phase 3: Runtime Dispatch (The "Hybrid" Model)

Loops with unknown sizes (like `Vec<T>`) should not always be vectorized (setup overhead).

**Strategy:** Compile two versions of the loop body (Scalar vs. Vectorized) and select at runtime.

```vex
// Pseudocode generated by compiler
if vec.len > 128 {
    // Jump to Validated Vector Loop (AVX-512 / SPIR-V)
    call vector_loop(vec)
} else {
    // Standard Scalar Loop
    call scalar_loop(vec)
}
```

## Phase 4: SPIR-V Offloading (SIMT)

For massive workloads (>1M items), map the loop logic to a SPIR-V compute shader.

1.  Extract loop body into a separate function.
2.  Transpile LLVM IR subset -> SPIR-V.
3.  Inject Vex Runtime call to dispatch to GPU.

## Phase 5: Deep LLVM Hints (Advanced)

Beyond vectorization, Vex can exploit its strong type system to provide LLVM with "Perfect Information".

### 1. Branch Prediction (`!prof`)
Error handling in Vex (`Result.Err`) is typically the "unlikely" path.
*   **Strategy:** Tag `if` conditions checking for errors with `branch_weights` metadata favouring the "happy path".
*   **Effect:** CPU branch predictor stays hot on the main logic trace.

### 2. Invariant Loads (`!invariant.load`)
Vex variables are immutable by default (`let`).
*   **Strategy:** Tag loads from immutable references with `!invariant.load`.
*   **Effect:** LLVM can aggressively hoist these loads out of loops, knowing they will never change, even if there are function calls inside the loop.

### 3. Alignment & Non-Null (`!align`, `!nonnull`)
Vex references (`&T`) are strictly non-null and properly aligned.
*   **Strategy:** Attach `!nonnull` and `!align` metadata to every load/store of a reference type.
*   **Effect:** Enables "aligned vector moves" (simpler assembly instructions) and removes redundant null checks.

### 4. Fast Math (Context Dependent)
 Inside `fn` or explicitly marked blocks, we can enable "Fast Math" flags (`nnan`, `nsz`, `arcp`).
*   **Effect:** 10-20% speedup on floating point heavy ML workloads by ignoring NaN corner cases and allowing algebraic re-association.

## Next Steps (Immediate)

Start with **Phase 1 (Loop Metadata)** and **Phase 5.3 (Alignment/NonNull)** as they yield the highest ROI with the lowest risk.

